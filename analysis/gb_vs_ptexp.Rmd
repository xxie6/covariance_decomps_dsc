---
title: "gb_vs_ptexp"
author: "Annie Xie"
date: "2025-09-04"
output: 
  workflowr::wflow_html:
    code_folding: hide
editor_options:
  chunk_output_type: console
---

# Introduction
In this analysis, I am interesting in comparing the additive-clustering style methods (e.g. the empirical Bayes methods with generalized binary prior) to the (sparse) symmetric NMF style methods (e.g. the empirical Bayes methods with point-exponential prior).

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)
library(pheatmap)
```

```{r}
source('code/visualization_functions.R')
```

# Prepare the DSC data

```{r}
dscout <- readRDS("data/dsc_results_df.rds")
dim(dscout)
```

I decided to focus on the backfit variant of the empirical Bayes matrix factorization methods. I also decided to focus on the variant of SINDCLUS and SYMPRES that does not explicitly model an intercept. So I clean the data to only include these variants.
```{r}
dscout <- dscout %>% filter((is.na(analyze.additive_term) == TRUE & is.na(analyze.backfit) == TRUE) | (analyze.additive_term == 'FALSE' | analyze.backfit == 'TRUE')) %>% select(!(analyze.off_diagonal))
```

I also filter out other settings we're not interested in (e.g. the input $K$ being misspecified).
```{r}
dscout <- dscout %>% filter(analyze.Kmax_factor == 1)
```

# Balanced Tree Setting
I will first consider the balanced tree setting. This setting is the hardest of the four settings due to the non-identifiability of the representation.

## Crossproduct Similarity

This is the average crossproduct similarity for each method in the balanced tree setting:
```{r}
dscout %>% filter(simulate == 'baltree_4pop', score == 'crossprod_similarity') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

For the empirical Bayes methods, the point-exponential variants performed worse than the generalized-binary variants (though in some cases it is only slightly worse). I don't think this is too surprising because this setting has non-identifiability issues, and I can imagine the point-exponential prior converging to factorizations where the factors are not close to binary.

## Proportion of true factors that are highly similar to an estimated factor

```{r}
dscout %>% filter(simulate == 'baltree_4pop', score == 'prop_true_high_cos_sim') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

Interestingly, for Flash with normal prior on F, using the previous metric it looked like point-exponential performed worse than generalized binary. But in this metric, it looks like the reverse is true. This is unexpected, and I don't have an explanation for this yet.

# Unbalanced Non-overlapping
I will now consider the unbalanced non-overlapping setting. This setting should be the easiest of the four settings.

## Crossproduct Similarity

This is the average crossproduct similarity for each method:
```{r}
dscout %>% filter(simulate == 'group_nonoverlap', simulate.pop_sizes != 'rep(40, 4)', score == 'crossprod_similarity') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

For most of the empirical Bayes methods, point-exponential performed the same or a little bit worse. However, for Flash with the normal prior on F, the point-exponential prior performed a little better than the generalized-binary prior. In my experience, the point-exponential prior is better with shrinking small values down to zero, so maybe that is why it is performing better? That fact is not unique to this method though, so this requires some more investigation. Furthermore, SINDCLUS and SYMPRES perform noticeably worse than CoDesymNMF in this setting.

## Proportion of true factors that are highly similar to an estimated factor

```{r}
dscout %>% filter(simulate == 'group_nonoverlap', simulate.pop_sizes != 'rep(40, 4)', score == 'prop_true_high_cos_sim') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

For Flash with normal prior on F, this metric also suggests that the point-exponential prior performs a little better than the generalized binary.

# Balanced Non-overlapping
I will now consider the balanced non-overlapping setting. This setting should be harder than the unbalanced non-overlapping setting.

## Crossproduct Similarity

This is the average crossproduct similarity for each method:
```{r}
dscout %>% filter(simulate == 'group_nonoverlap', simulate.pop_sizes == 'rep(40, 4)', score == 'crossprod_similarity') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

We see a similar phenomenon to the unbalanced non-overlapping case -- for most of the empirical Bayes methods, point-exponential performed the same or a little bit worse. However, for Flash with the normal prior on F, the point-exponential prior performed a little better than the generalized-binary prior. Again, SINDCLUS and SYMPRES perform noticeably worse than CoDesymNMF in this setting.

## Proportion of true factors that are highly similar to an estimated factor

```{r}
dscout %>% filter(simulate == 'group_nonoverlap', simulate.pop_sizes == 'rep(40, 4)', score == 'prop_true_high_cos_sim') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

We again see a similar phenomenon to the unbalanced non-overlapping case.

# Sparse Overlapping
I will now consider the sparse, overlapping setting.

## Crossproduct Similarity

This is the average crossproduct similarity for each method:
```{r}
dscout %>% filter(simulate == 'group_overlap', score == 'crossprod_similarity') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

Interestingly, for many of the empirical Bayes methods, the point-exponential prior had better performance. The only EB method where this is not the case is EBMFcov.

## Proportion of true factors that are highly similar to an estimated factor

```{r}
dscout %>% filter(simulate == 'group_overlap', score == 'prop_true_high_cos_sim') %>% group_by(analyze, analyze.ebnm_fn) %>% summarise(avg_result = mean(score.result)) %>% arrange(analyze, analyze.ebnm_fn)
```

We see a similar trend to what we saw with the crossproduct similarity in this case -- for many of the EB methods, the point-exponential prior has better performance (in terms of proportion of recovery).
